# 作业五  使用tf2构建SEBlock+LeNet

<center>2020年4月8日</center>

| 年级 | 学号       | 姓名   | 指导老师 |
| ---- | ---------- | ------ | -------- |
| 2020 | 2016012963 | 董佩杰 | 牛新     |

## 1. 实验要求

 SENet (Squeeze-Excitation Net),其核心为设计SENetBlock去评估feature各个通道的权重，并用该权重给feature加权，用加权的feature当作实际的feature输出。 

![SE Block示意图](https://img-blog.csdnimg.cn/20200101094228695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70)

SENetBlock是一个很通用的模块，结构其实非常容易实现，也易于添加到我们自己的网络结构中去，提升现有效果。我们只需要在相关的层后面，添加一个SENetBlock层就可以了。

![SE模块内部实现](dongpeijie_2016012963_5.assets/1586308789052.png)

要求：

1. 完成以上代码设计，并将运行部分和实验结果部分进行截图。
2. 将SENet-LeNet模型使用plot_model绘制出来。
3. 将SE Block中提取得到的16维的特征进行可视化。



## 2. 实验过程

### 2.1 SeNetBlock构建

由于使用了全局平均池化，得到的结果可以通过Conv1D或者Dense模块进行特征提取。

```python
def SeNetBlock(feature, reduction=4):
    temp = feature
    channel_axis = 1 if K.image_data_format() == "channels_first" else -1
    channels = temp.shape[channel_axis]
    # 得到feature的通道数量w
    avg_x = tf.keras.layers.GlobalAveragePooling2D()(temp)
    # 先对feature的每个通道进行全局平均池化Global Average Pooling 得到通道描述子（Squeeze）
    x = tf.keras.layers.Dense(
        units=int(channels)//reduction, activation=tf.nn.relu, use_bias=False)(avg_x)
    # 接着做reduction，用int(channels)//reduction个卷积核对 avg_x做1x1的点卷积
    x = tf.keras.layers.Dense(units=int(channels), use_bias=False)(x)
    # 接着用int(channels)个卷积核个数对 x做1x1的点卷积，扩展x回到原来的通道个数
    se_feature = tf.keras.activations.sigmoid(x)  # 对x 做 sigmoid 激活
    return multiply([se_feature, feature]), se_feature
    # 返回以cbam_feature 为scale，对feature做拉伸加权的结果（Excitation）
```

### 2.2 SeNet-LeNet构建

按照要求，对第二个卷积的结果施加SE模块，进行特征重标定。

```python
input = Input(shape=(28, 28, 1))
x = Conv2D(6, (5, 5), (1, 1), activation='relu')(input)
x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)
x = Conv2D(16, (5, 5), (1, 1),
           activation='relu', padding='valid')(x)
# SE Module
x, se_feature = SeNetBlock(x)
x = MaxPooling2D(2, strides=(2, 2))(x)
x = Flatten()(x)
x = Dense(120, activation='relu')(x)
x = Dense(84, activation='relu')(x)
x = Dense(10, activation='softmax')(x)
model = Model(input, [x, se_feature], name="SELeNet")
model.summary()
```

![模型的Summary](dongpeijie_2016012963_5.assets/1586309338168.png)

### 2.3 绘制SeLeNet结构

通过调用：

```python
plot_model(model, to_file="SELeNet.png")
```

完成绘制，在这个过程中需要注意安装pydot, graphviz等安装包，并将graphviz的bin添加到path中才能正常绘制。

绘制结果如下:

![SELeNet](dongpeijie_2016012963_5.assets/SELeNet.png)

### 2.4 训练和测试流程

训练和测试流程和之前几次作业内容一致：

```python

@tf.function
def train_step(images, labels):
    with tf.GradientTape() as tape:
        predictions, se_feature = model(images)
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)


@tf.function
def test_step(images, labels):
    predictions, se_feature = model(images)
    t_loss = loss_object(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)
    return se_feature

#################### 5. train ####################

for epoch in range(EPOCHS):
    train_loss.reset_states()
    train_accuracy.reset_states()
    test_loss.reset_states()
    test_accuracy.reset_states()

    for images, labels in train_ds:
        train_step(images, labels)

    for test_images, test_labels in test_ds:
        se_feature = test_step(test_images, test_labels)

    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
    print(template.format(epoch+1,
                          train_loss.result(),
                          train_accuracy.result()*100,
                          test_loss.result(),
                          test_accuracy.result()*100))

```

其中修改了test_step，可以让模型返回se_feature，方便最后的可视化。

以下样本训练使用的是Adam, 默认lr=0.001，batch size=100, epoch=10

![训练日志](dongpeijie_2016012963_5.assets/1586310330591.png)

训练样本上能达到99%+的准确率，测试集上能达到98%+的准确率。

如果使用任务中推荐的参数，SGD+lr=0.01, batch size=100, epoch=10

![任务参数得到的训练日志](dongpeijie_2016012963_5.assets/1586310760705.png)

最终只能得到大概97%+的准确率。

### 2.5 SE feature 可视化

```python
for i, (images, labels) in enumerate(test_ds):
    se_feature = test_step(images, labels)
    labels = tf.cast(labels, tf.int32)
    sorted = tf.argsort(labels)
    labels = tf.gather(labels, sorted)
    se_feature = tf.gather(se_feature, sorted)

    se_feature_numpy = se_feature.numpy()

    plt.figure()
    img = plt.imshow(se_feature_numpy)
    img.set_cmap('hot')
    plt.savefig("./%d_se_feature.png" % i)
    plt.close()
```

通过设置超参数batch size为100，而每个batch样本通过debug发现几乎是均衡的，所以一个batch内部10个类别格式也是均衡的，所以需要做的就是排序。

tensorflow2中还不能直接使用numpy中的API, 比如argsort，但是经过查询tensorflow2的API可以找到tf中封装好的函数。将tf.argsort得到的排序index施加到se_feature所代表的数组也需要使用规定的API: tf.gather。

在这个部分遇到了很多bug，比如如果不将labels设置为tf.int32格式，就会报错。如果使用类似numpy的方式分配索引排序也会报错。

最后将多个batch的结果保存下来，使用matplotlib中的热图将其显示出来，得到以下结果：

![可视化se中的feature](dongpeijie_2016012963_5.assets/38_se_feature.png)

经过观察，得到以下结论：

- 每次重新训练得到的feature结果都不太一致，条纹分布不一致
- 不同测试样本中呈现的条纹状
- 设置epoch不同对应的feature结果也不一致

原因可能是：

- 权重初始化不同，从而产生的每次训练得到的feature map不同，epoch也同理
- 条纹状则是由于样本本身存在的随机性导致的条纹，每个batch中样本都存在不同分布

### 2.6 扩展实验

为了验证是否是初始化不同导致的，固定代码中不确定因素：

```python
np.random.seed(0)
np.set_printoptions(precision=4)

tf.random.set_seed(0)
```

这样连续进行两次实验，而且是同一个feature map的结果如下：

![前后两次feature map是一致的](dongpeijie_2016012963_5.assets/1586312298371.png)