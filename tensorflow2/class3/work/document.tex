% -*- coding: utf-8 -*-
\documentclass{article}
\usepackage{listings}
\usepackage{ctex}
\usepackage{graphicx}
\usepackage[a4paper, body={18cm,22cm}]{geometry}
\usepackage{amsmath,amssymb,amstext,wasysym,enumerate,graphicx}
\usepackage{float,abstract,booktabs,indentfirst,amsmath}
\usepackage{array}
\usepackage{booktabs} %调整表格线与上下内容的间隔
\usepackage{multirow}
\usepackage{url}
\usepackage{diagbox}
\renewcommand\arraystretch{1.4}
\usepackage{indentfirst}
\setlength{\parindent}{2em}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
	numbers=left, 
	numberstyle= \tiny, 
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50}, 
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
	basicstyle=\footnotesize,
	framexleftmargin=2em
} 


\geometry{left=2.8cm,right=2.2cm,top=2.5cm,bottom=2.5cm}
%\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}

\graphicspath{{figures/}}

\title{\heiti  深度学习方法与实践 实验三}

\begin{document}
    \maketitle
    %\tableofcontents
    
    \begin{center}

        %\date{2019年4月2日}
        \begin{table}[H]
            \centering
            \begin{tabular}{p{3cm}p{4cm}<{\centering}p{3cm}p{4cm}<{\centering}}
                年\quad 级: & 2020级 & 学\qquad 号:   & 2016012963 \\ \cline{2-2} \cline{4-4} 
                姓\quad 名: & 董佩杰     & 指导老师: &  牛新       \\ \cline{2-2} \cline{4-4} 
            \end{tabular}
        \end{table}
    \end{center}
    
    \section{设计变量共享网络进行MNIST分类}
    
    \subsection{实验要求}
    
    其将图片样本分为上下两半X1,X2；分别送入input1,input2。后续的两个路径的线性加权模块 share\_base(X)=X*W 
    共享一个变量 name='w'
    
    整个分类模型可描述为 softmax(share\_base(X1)+share\_base(X2)+b)
    
    注意：b 是一个变量，share\_base是一层全连接，没有偏置
    
    网络结构如下所示:
    
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=0.7\linewidth]{153d1cd92b5c}
    	\caption{网络结构图}
    	\label{fig:153d1cd92b5c}
    \end{figure}


	\subsection{具体实现}
	
	\begin{itemize}
		\item 必须实现要求的共享网络结构，图像是上下分半，网络能够打印出类似下方给的网络结构图。
		\item 训练分类精度上0.85以上
	\end{itemize}

	1. 模型部分实现
	
\begin{lstlisting}[language=Python]
inputs = Input(shape=(392, ), name="D1_input")
outputs = Dense(num_classes, name="D1")(inputs)
share_base = Model(inputs=inputs, outputs=outputs, name="seq1")

x1 = Input(shape=(392, ), name="input_1")
x2 = Input(shape=(392, ), name="input_2")
s1 = share_base(x1)
s2 = share_base(x2)

b = K.zeros(shape=(10))
x = s1 + s2 + b
x = Activation('softmax', name='activation')(x)

siamese_net = Model(inputs=[x1, x2], outputs=x)
\end{lstlisting}

	2. 使用plot\_model绘制模型结构
    
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=0.7\linewidth]{siamese_net}
    	\caption{keras绘制出模型结构}
    	\label{fig:siamesenet}
    \end{figure}
    
    3. 对网络层中的变量进行可视化
    
    D1对应的权重形状应为392x10, 所以可视化的时候分别可视化10个14x28=392大小的weights。
    
    
\begin{lstlisting}[language=Python]
train_weights=siamese_net.get_layer('seq1').get_layer('D1').kernel.numpy()

print(train_weights.shape)

num = np.arange(0, 392, 1, dtype="float")
num = num.reshape((14, 28))
plt.figure(num='Weights', figsize=(10, 10))  
# 创建一个名为Weights的窗口,并设置大小
for i in range(10):  # W.shape[1]
num = train_weights[:, i: i+1].reshape((14, -1))
plt.subplot(2, 5, i + 1)
num = num * 255.
plt.imshow(num, cmap=plt.get_cmap('hot'))
plt.title('weight %d image.' % (i + 1))  # 第i + 1幅图片
plt.show()
\end{lstlisting}
    
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=0.7\linewidth]{Weights}
    	\caption{可视化结果}
    	\label{fig:weights}
    \end{figure}
    
    
    4. 训练输出log
    
    
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=0.7\linewidth]{log1}
    	\caption{训练log}

    	\label{fig:log1}
    \end{figure}
    
    
    5. 结果以及全部代码
    
    模型可以在10个epoch以内训练集和测试集均能达到90\%的准确率。
    
\begin{lstlisting}[language=Python]
import os
import time

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model, Sequential
from tensorflow.keras import backend as K
from tensorflow.keras.layers import (Activation, Conv2D, Dense, Flatten, Input,
concatenate)
from tensorflow.keras.utils import plot_model

num_classes = 10
total_epoch = 30
mnist = tf.keras.datasets.mnist

#1. prepare datasets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train / 255.0
x_test = x_test / 255.0

x_train = x_train.reshape(x_train.shape[0], 2, -1)
x_test = x_test.reshape(x_test.shape[0], 2, -1)

y_train = tf.one_hot(y_train, num_classes)
y_test = tf.one_hot(y_test, num_classes)

train_ds = tf.data.Dataset.from_tensor_slices(
(x_train, y_train)).shuffle(1000).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

#2. net_build
inputs = Input(shape=(392, ), name="D1_input")
outputs = Dense(num_classes, name="D1")(inputs)
share_base = Model(inputs=inputs, outputs=outputs, name="seq1")

x1 = Input(shape=(392, ), name="input_1")
x2 = Input(shape=(392, ), name="input_2")
s1 = share_base(x1)
s2 = share_base(x2)

b = K.zeros(shape=(10))
x = s1 + s2 + b
x = Activation('softmax', name='activation')(x)

siamese_net = Model(inputs=[x1, x2], outputs=x)

plot_model(siamese_net,to_file='./siamese_net.png',show_shapes=True,
expand_nested=True)

#3. train and test
loss_ce = tf.keras.losses.categorical_crossentropy
optimizer = tf.keras.optimizers.Adam(3e-4)

# metrics用于记录指标
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_acc = tf.keras.metrics.CategoricalAccuracy(name='train_acc')
test_loss = tf.keras.metrics.Mean(name='test_loss')
test_acc = tf.keras.metrics.CategoricalAccuracy(name='test_loss')

def train_step(images, labels):
	part1 = images[:, 0]
	part2 = images[:, 1]
	with tf.GradientTape() as tape:
		outputs = siamese_net([part1, part2])
		loss = loss_ce(labels, outputs)
	gradients = tape.gradient(loss, siamese_net.trainable_variables)
	optimizer.apply_gradients(zip(gradients, siamese_net.trainable_variables))
	
	train_loss(loss)
	train_acc(labels, outputs)

def test_step(images, labels):
	part1 = images[:, 0]
	part2 = images[:, 1]
	outputs = siamese_net([part1, part2])
	loss = loss_ce(labels, outputs)

	test_loss(loss)
	test_acc(labels, outputs)


for epoch in range(total_epoch):
	train_acc.reset_states()
	train_loss.reset_states()
	test_acc.reset_states()
	test_loss.reset_states()
	for images, labels in train_ds:
		train_step(images, labels)
	
	for images, labels in test_ds:
		test_step(images, labels)
	
	print(
	"epoch:%d,train loss:%.3f,train acc:%.3f,test loss:%.3f,test acc:%.3f"
	% (epoch, train_loss.result(), train_acc.result() * 100,
	test_loss.result(), test_acc.result() * 100))

#4. draw weights of 10 classes
train_weights=siamese_net.get_layer('seq1').get_layer('D1').kernel.numpy()

print(train_weights.shape)

num = np.arange(0, 392, 1, dtype="float")
num = num.reshape((14, 28))
plt.figure(num='Weights', figsize=(10, 10))  # 创建一个名为Weights的窗口,并设置大小
for i in range(10):  # W.shape[1]
	num = train_weights[:, i: i+1].reshape((14, -1))
	plt.subplot(2, 5, i + 1)
	num = num * 255.
	plt.imshow(num, cmap=plt.get_cmap('hot'))
	plt.title('weight %d image.' % (i + 1))  # 第i + 1幅图片
plt.show()
print(np.min(num))
print(np.max(num))

\end{lstlisting}

        

\section{连体网络MINIST识别}

	\subsection{实验要求}
	
		构建如下图所示识别模型：该模型由两个相同的网络G(x)组成。两个网络共享相同的参数W.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{image}
			\caption{网络结构}
			\label{fig:image}
		\end{figure}
	
		该模型实现如下的功能，输入两个MINIST图片，判断是不是同一个数字。
		
		例如，输入  负样本对：X1=6的图片 ， X2=9的图片 输出：1;		
		输入  正样本对：X1=3的图片 ， X2=3的图片 输出：0
		
		G(x)是一个一般的全连接网络（两边的网络结构是一样的！共享参数W、b等），由结构可以自己设计。比如建议两层网络：hidden1：784(28x28)->500; hidden2: 500->10，使用relu。也可以尝试其他节点数组合，和其他非线性变换函数。
		
		强调：G(X）的功能定义为提取一张mnist图像的特征。
		
		该模型的训练采用如下损失函数：
		\begin{align*}
			L(W,Y,X_1,X_2) &= (1-Y)L_{G}(E_W)+YL_1(E_W) 
			\\
			&= (1-Y)\frac{2}{Q}(E_W)^2 + (Y)2Qe^{-\frac{2.77}{Q}E_W}
		\end{align*}
		
		其中Q是一个常数，用于控制正负样本的平衡，类似于focal loss。
		
		$E_W$是两个网络输出特征的$L_2$距离，$E_W=||G_W(X_1)-G_W(X_2)||$

	
	\subsection{具体实现}
	
	
	1. 模型实现
	
	  构造一个孪生网络，其中获得的embedding是一个10维的向量。
	
\begin{lstlisting}[language=Python]
class MyModel(Model):
	def __init__(self):
		super(MyModel, self).__init__()
		self.d1 = Dense(500, activation='relu')
		self.d2 = Dense(10, activation='softmax')
		
	def call(self, x):
		x = self.d1(x)
		embedding = self.d2(x)
		return embedding
\end{lstlisting}


  
  	2. 数据加载
  	
  	数据的label是严重不平衡的，所以数据处理部分使用均衡采样的方法来让正样本和负样本比例为1:1。具体代码如下：
  	
  	
\begin{lstlisting}[language=Python]
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

train_ds = tf.data.Dataset.from_tensor_slices(
(x_train, y_train)).shuffle(1000).batch(BATCH_SIZE)

test_ds = tf.data.Dataset.from_tensor_slices(
(x_test, y_test)).batch(BATCH_SIZE)


def balance_sample(train_ds, test_ds, train=True):
	train_ds = iter(train_ds)
	test_ds = iter(test_ds)
	if train:
		x1, y1 = next(train_ds)
		x2, y2 = next(train_ds)
	else:
		x1, y1 = next(test_ds)
		x2, y2 = next(test_ds)
		
	y1 = y1[..., np.newaxis]
	y2 = y2[..., np.newaxis]
	
	idx_same = np.where(y1 == y2) # 找到相同的下角标
	idx_rand = np.random.randint(BATCH_SIZE, size=len(idx_same))  # 随机取样
	index = np.union1d(idx_same, idx_rand).astype(np.int64)  # 所有需要取样的样本
	
	data_list = []
	label_list = []
	
	judge = np.array(y1 != y2)
	
	for ix in index:
		data_list.append([x1[ix], x2[ix]])
		label_list.append(judge[ix])
		
	return np.array(data_list), np.array(label_list)
\end{lstlisting}

	3. Loss部分具体实现
	
	Loss部分主要有两个方法，一个是计算两个embedding的距离，一个是实现要求中的loss，其中需要说明的是这里的Q取1,由于采样的时候采用的是均衡采样，正负样本比例为1:1，所以不需要调整Q。
	
\begin{lstlisting}[language=Python]

def dist(output1, output2):
	E = K.sqrt(K.sum(K.square(output1 - output2), 1))  # dim=1
	return E
	

def loss_object(Y, E, Q=1):
	pos_loss = Y * 2 * Q * K.exp((-2.77 * E) / Q)
	neg_loss = 2 * (1 - Y) * (E**2) / Q
	return pos_loss + neg_loss

\end{lstlisting}

	
	4. 训练log
	
	通过调参(主要是batch size和learning rate), 模型结果很快能达到97\%以上。
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\linewidth]{log2}
		\caption{训练log}
		\label{fig:log2}
	\end{figure}

	
	5. 全部代码
	
	
\begin{lstlisting}[language=Python]
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Conv2D, Dense, Flatten

tf.keras.backend.set_floatx('float64')
mnist = keras.datasets.mnist

######################
EPOCHS = 100
BATCH_SIZE = 1000
LEN_IMAGE_SIZE = 784

lr = 3e-4
e_w = 1.0
iters = 5
######################

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

train_ds = tf.data.Dataset.from_tensor_slices(
(x_train, y_train)).shuffle(1000).batch(BATCH_SIZE)

test_ds = tf.data.Dataset.from_tensor_slices(
(x_test, y_test)).batch(BATCH_SIZE)


def balance_sample(train_ds, test_ds, train=True):
	train_ds = iter(train_ds)
	test_ds = iter(test_ds)
	if train:
		x1, y1 = next(train_ds)
		x2, y2 = next(train_ds)
	else:
		x1, y1 = next(test_ds)
		x2, y2 = next(test_ds)
	
	y1 = y1[..., np.newaxis]
	y2 = y2[..., np.newaxis]
	
	idx_same = np.where(y1 == y2) # 找到相同的下角标
	idx_rand = np.random.randint(BATCH_SIZE, size=len(idx_same))  # 随机取样
	index = np.union1d(idx_same, idx_rand).astype(np.int64)  # 所有需要取样的样本
	
	data_list = []
	label_list = []
	
	judge = np.array(y1 != y2)
	
	for ix in index:
		data_list.append([x1[ix], x2[ix]])
		label_list.append(judge[ix])
	
	return np.array(data_list), np.array(label_list)


class MyModel(Model):
	def __init__(self):
		super(MyModel, self).__init__()
		self.d1 = Dense(500, activation='relu')
		self.d2 = Dense(10, activation='softmax')
		
	def call(self, x):
		x = self.d1(x)
		embedding = self.d2(x)
		return embedding


def dist(output1, output2):
	E = K.sqrt(K.sum(K.square(output1 - output2), 1))  # dim=1
	return E


def loss_object(Y, E, Q=1):
	pos_loss = Y * 2 * Q * K.exp((-2.77 * E) / Q)
	neg_loss = 2 * (1 - Y) * (E**2) / Q
	return pos_loss + neg_loss


model = MyModel()

optimizer = tf.keras.optimizers.Adam(lr)

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')

def train_epoch(train_ds):
	for i in range(iters):
		with tf.GradientTape() as tape:
			data, label = balance_sample(train_ds, test_ds, train=True)
			output1 = model(data[:, 0])
			output2 = model(data[:, 1])
			E = dist(output1, output2)
			
			label = np.squeeze(label, 1)
			
			loss = loss_object(label, E)
		gradients = tape.gradient(loss, model.trainable_variables)
		optimizer.apply_gradients(zip(gradients, model.trainable_variables))
		E = K.cast(E >= e_w, dtype='float64')
		train_loss(loss)
		train_accuracy(label, E)

def test_epoch(test_ds):
	for i in range(iters):
		data, label = balance_sample(train_ds, test_ds, train=False)
		output1 = model(data[:, 0])
		output2 = model(data[:, 1])
		E = dist(output1, output2)
		loss = loss_object(label, E)
		E = K.cast(E >= e_w, dtype='float64')
		test_loss(loss)
		test_accuracy(label, E)

for epoch in range(EPOCHS):
	train_loss.reset_states()
	train_accuracy.reset_states()
	test_loss.reset_states()
	test_accuracy.reset_states()

	train_epoch(train_ds)
	test_epoch(test_ds)
	
	print('Epoch {}, Train Loss: {:.3f}, Train acc: {:.3f}, Test Loss: {:.3f} Test acc: {:.3f}'.
    format(epoch + 1, train_loss.result(), train_accuracy.result(),
	test_loss.result(), test_accuracy.result()))

\end{lstlisting}   
        
    
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%Library%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 1. 脚注用法
    LaTeX\footnote{Latex is Latex} is a good software


%L(W,Y,X_1,X_2) = (1-Y)L_{G}(E_W)+YL_1(E_W) \\
%= (1-Y)\frac{2}{Q}(E_W)^2 + (Y)2Qe^{-\frac{2.77}{Q}E_W}

%2. 强调
    \emph{center of percussion} %[Brody 1986], %\lipsum[5]

%3. 随便生成一段话
    \lipsum[4]

%4. 列条目
    \begin{itemize}
    \item the angular velocity of the bat,
    \item the velocity of the ball, and
    \item the position of impact along the bat.
    \end{itemize}

%5. 表格用法
    \begin{table}[h]
    \centering  
    \begin{tabular}{c|cc}
        \hline
        年份 & \multicolumn{2}{c}{指标}\\
        \hline
        2017 & 0.9997 & 0.0555 \\
        2018 & 0.9994 & 0      \\
        2019 & 0.9993 & 0      \\
        \hline
    \end{tabular}
    \caption{NAME}\label{SIGN}
    \end{table}

    \begin{center}
        \begin{tabular}{c|cclcrcc}
            \hline
            Year & theta & $S_1^-$ & $S_2^-$ & $S_3^-$ & $S_4^+$ & $S_5^+$ & $S_6^+$ \\%表格标题
            \hline
            2016 & 1      & 0      & 0 & 0.0001 & 0      & 0      & 0 \\
            2017 & 0.9997 & 0.0555 & 0 & 0.2889 & 0.1844 & 0.463  & 0 \\
            2018 & 0.9994 & 0      & 0 & 0.0012 & 0.3269 & 0.7154 & 0 \\
            2019 & 0.9993 & 0      & 0 & 0      & 0.4325 & 1.0473 & 0 \\
            2020 & 0.9991 & 0      & 0 & 0      & 0.5046 & 1.2022 & 0 \\
            2021 & 0.999  & 0      & 0 & 0      & 0.5466 & 1.2827 & 0 \\
            2022 & 0.9989 & 0.0017 & 0 & 0.3159 & 0.562  & 1.2995 & 0 \\
            2023 & 0.9989 & 0      & 0 & 0.0109 & 0.5533 & 1.2616 & 0 \\
            2024 & 0.9989 & 0      & 0 & 0      & 0.5232 & 1.1769 & 0 \\
            2025 & 0.9989 & 0      & 0 & 0.1009 & 0.4738 & 1.0521 & 0 \\
            2026 & 0.9991 & 0      & 0 & 0      & 0.4071 & 0.8929 & 0 \\
            2027 & 0.9992 & 0.0004 & 0 & 0.1195 & 0.3248 & 0.7042 & 0 \\
            2028 & 0.9994 & 0.0164 & 0 & 0.046  & 0.2287 & 0.4902 & 0 \\
            2029 & 0.9997 & 0      & 0 & 0.0609 & 0.12   & 0.2545 & 0 \\
            2030 & 1      & 0      & 0 & 0      & 0      & 0      & 0 \\
            \hline
        \end{tabular}
    \end{center}

%6. 数学公式
    \begin{equation}
        a^2 = a * a\label{aa}
    \end{equation}
    
    \[
    \begin{pmatrix}{*{20}c}
    {a_{11} } & {a_{12} } & {a_{13} }  \\
    {a_{21} } & {a_{22} } & {a_{23} }  \\
    {a_{31} } & {a_{32} } & {a_{33} }  \\
    \end{pmatrix}
    = \frac{{Opposite}}{{Hypotenuse}}\cos ^{ - 1} \theta \arcsin \theta
    \]
    
    \[
    p_{j}=\begin{cases} 0,&\text{if $j$ is odd}\\
    r!\,(-1)^{j/2},&\text{if $j$ is even}
    \end{cases}
    \]
    
    
    \[
    \arcsin \theta  =
    \mathop{{\int\!\!\!\!\!\int\!\!\!\!\!\int}\mkern-31.2mu
        \bigodot}\limits_\varphi
    {\mathop {\lim }\limits_{x \to \infty } \frac{{n!}}{{r!\left( {n - r}
                \right)!}}} \eqno (1)
    \]

%7. 双图并行
    \begin{figure}[h]
        % 一个2*2图片的排列
        \begin{minipage}[h]{0.5\linewidth}
            \centering
            \includegraphics[width=0.8\textwidth]{./figures/0.jpg}
            \caption{Figure example 2}
        \end{minipage}
        \begin{minipage}[h]{0.5\linewidth}
            \centering
            \includegraphics[width=0.8\textwidth]{./figures/0.jpg}
            \caption{Figure example 3}
        \end{minipage}
    \end{figure}

%8. 单张图片部分
    \begin{figure}[h]
        %\small
        \centering
        \includegraphics[width=12cm]{./figures/mcmthesis-aaa.eps}
        \caption{Figure example 1} \label{fig:aa}
    \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{minipage}{0.5\linewidth}
    \begin{tabular}{|c|c|c|}
        \hline
        \multicolumn{2}{|c|}{\multirow{2}{*}{合并}}&测试\\
        \cline{3-3}
        \multicolumn{2}{|c|}{}& 0.9997  \\
        \hline
        2019 & 0.9993 & 0 \\
        \hline
    \end{tabular}
\end{minipage}

\begin{minipage}{0.5\linewidth}
    \begin{tabular}{c|ccc}
        \hline
        年份 & \multicolumn{3}{c}{指标}\\
        \hline
        \multirow{3}{*}{合并}&2017 & 0.9997 & 0.0555 \\
        &2018 & 0.9994 & 0      \\
        &2019 & 0.9993 & 0      \\
        \hline
    \end{tabular}
\end{minipage}



    \begin{table}[h]
    \centering  
    \begin{Large}
        \begin{tabular}{p{4scm} p{8cm} < {\centering}}
            \hline
            院\qquad 系: & 信息工程学院 \\
            \hline
            团队名称: & PlantBook Team \\
            \hline
            分\qquad 组: & 第0组1号 \\
            \hline
            日\qquad 期: & 2017年10月28日 \\
            \hline
            指导教师: & 吱吱吱\\
            \hline
        \end{tabular}
    \end{Large}
\end{table}


\ctexset{
    section={
        format+=\heiti \raggedright,
        name={,、},
        number=\chinese{section},
        beforeskip=1.0ex plus 0.2ex minus .2ex,
        afterskip=1.0ex plus 0.2ex minus .2ex,
        aftername=\hspace{0pt}
    },
}

    \begin{table}[h]
    \centering
    \begin{Large}
        \begin{tabular}{p{3cm} p{7cm}<{\centering}}
            院  \qquad  系: & ***          \\ \cline{2-2}
        \end{tabular}
    \end{Large}     
\end{table}
\thispagestyle{empty}
\newpage
\thispagestyle{empty}
\tableofcontents
\thispagestyle{empty}
\newpage
\setcounter{page}{1}