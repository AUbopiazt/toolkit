{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sx9e_pXlCuti"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMMut8UVCutt"
   },
   "source": [
    "# Experiments\n",
    "We'll go through learning feature embeddings using different loss functions on MNIST dataset. This is just for visualization purposes, thus we'll be using 2-dimensional embeddings which isn't the best choice in practice.\n",
    "\n",
    "For every experiment the same embedding network is used (32 conv 5x5 -> PReLU -> MaxPool 2x2 -> 64 conv 5x5 -> PReLU -> MaxPool 2x2 -> Dense 256 -> PReLU -> Dense 256 -> PReLU -> Dense 2) and we don't do any hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcmGBqXeCutw"
   },
   "source": [
    "# Prepare dataset\n",
    "We'll be working on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AQSHPH9P0BNx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████▉| 9912320/9912422 [04:30<00:00, 31671.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                        | 0/28881 [00:00<?, ?it/s]\n",
      " 57%|█████████████████████████████████████████▍                               | 16384/28881 [00:01<00:00, 68777.18it/s]\n",
      "32768it [00:01, 48491.26it/s]                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                      | 0/1648877 [00:00<?, ?it/s]\n",
      "  1%|▋                                                                      | 16384/1648877 [00:00<00:26, 62257.18it/s]\n",
      "  2%|█▍                                                                     | 32768/1648877 [00:01<00:40, 39536.90it/s]\n",
      "  3%|██▍                                                                    | 57344/1648877 [00:01<00:33, 47962.91it/s]\n",
      "  4%|███▏                                                                   | 73728/1648877 [00:02<00:30, 52299.57it/s]\n",
      "  5%|███▌                                                                   | 81920/1648877 [00:02<00:35, 44755.00it/s]\n",
      "  6%|████▏                                                                  | 98304/1648877 [00:02<00:31, 49014.08it/s]\n",
      "  7%|████▊                                                                 | 114688/1648877 [00:02<00:29, 52857.41it/s]\n",
      "  8%|█████▌                                                                | 131072/1648877 [00:03<00:26, 56324.76it/s]\n",
      "  9%|██████▎                                                               | 147456/1648877 [00:03<00:39, 38295.21it/s]\n",
      " 10%|███████▎                                                              | 172032/1648877 [00:04<00:31, 46628.70it/s]\n",
      " 11%|███████▋                                                              | 180224/1648877 [00:04<00:35, 41750.71it/s]\n",
      " 12%|████████▎                                                             | 196608/1648877 [00:04<00:31, 46561.60it/s]\n",
      " 13%|█████████                                                             | 212992/1648877 [00:04<00:28, 50711.11it/s]\n",
      " 13%|█████████▍                                                            | 221184/1648877 [00:05<00:32, 44054.49it/s]\n",
      " 14%|██████████                                                            | 237568/1648877 [00:05<00:28, 49126.61it/s]\n",
      " 15%|██████████▊                                                           | 253952/1648877 [00:05<00:26, 52295.29it/s]\n",
      " 16%|███████████▍                                                          | 270336/1648877 [00:05<00:24, 55356.55it/s]\n",
      " 17%|████████████▏                                                         | 286720/1648877 [00:06<00:23, 58823.36it/s]\n",
      " 18%|████████████▊                                                         | 303104/1648877 [00:06<00:28, 47074.78it/s]\n",
      " 20%|█████████████▉                                                        | 327680/1648877 [00:06<00:23, 56062.43it/s]\n",
      " 21%|██████████████▌                                                       | 344064/1648877 [00:07<00:22, 58145.69it/s]\n",
      " 21%|██████████████▉                                                       | 352256/1648877 [00:07<00:27, 46945.56it/s]\n",
      " 22%|███████████████▋                                                      | 368640/1648877 [00:07<00:24, 51699.71it/s]\n",
      " 23%|████████████████▎                                                     | 385024/1648877 [00:08<00:28, 43765.53it/s]\n",
      " 25%|█████████████████▍                                                    | 409600/1648877 [00:08<00:23, 52708.68it/s]\n",
      " 26%|██████████████████                                                    | 425984/1648877 [00:08<00:21, 55628.07it/s]\n",
      " 26%|██████████████████▍                                                   | 434176/1648877 [00:09<00:37, 32651.62it/s]\n",
      " 28%|███████████████████▍                                                  | 458752/1648877 [00:09<00:34, 34769.16it/s]\n",
      " 29%|████████████████████▏                                                 | 475136/1648877 [00:10<00:30, 39023.50it/s]\n",
      " 29%|████████████████████▌                                                 | 483328/1648877 [00:10<00:43, 26612.09it/s]\n",
      " 30%|████████████████████▊                                                 | 491520/1648877 [00:10<00:41, 28057.82it/s]\n",
      " 30%|█████████████████████▏                                                | 499712/1648877 [00:11<00:49, 23322.36it/s]\n",
      " 31%|█████████████████████▌                                                | 507904/1648877 [00:11<00:51, 22352.18it/s]\n",
      " 31%|█████████████████████▉                                                | 516096/1648877 [00:12<00:55, 20293.83it/s]\n",
      " 32%|██████████████████████▎                                               | 524288/1648877 [00:12<00:49, 22752.43it/s]\n",
      " 32%|██████████████████████▌                                               | 532480/1648877 [00:12<00:44, 25246.73it/s]\n",
      " 33%|██████████████████████▉                                               | 540672/1648877 [00:13<00:51, 21655.87it/s]\n",
      " 34%|███████████████████████▋                                              | 557056/1648877 [00:13<00:40, 27231.90it/s]\n",
      " 34%|███████████████████████▉                                              | 565248/1648877 [00:13<00:38, 28370.66it/s]\n",
      " 35%|████████████████████████▎                                             | 573440/1648877 [00:13<00:36, 29400.53it/s]\n",
      " 35%|████████████████████████▋                                             | 581632/1648877 [00:14<00:34, 30657.32it/s]\n",
      " 36%|█████████████████████████▍                                            | 598016/1648877 [00:14<00:28, 36392.84it/s]\n",
      " 37%|█████████████████████████▋                                            | 606208/1648877 [00:14<00:38, 26773.69it/s]\n",
      " 38%|██████████████████████████▍                                           | 622592/1648877 [00:15<00:31, 32645.31it/s]\n",
      " 38%|██████████████████████████▊                                           | 630784/1648877 [00:15<00:37, 27421.86it/s]\n",
      " 39%|███████████████████████████▍                                          | 647168/1648877 [00:15<00:31, 31438.35it/s]\n",
      " 40%|███████████████████████████▊                                          | 655360/1648877 [00:16<00:38, 25971.29it/s]\n",
      " 40%|████████████████████████████▏                                         | 663552/1648877 [00:16<00:34, 28516.87it/s]\n",
      " 41%|████████████████████████████▌                                         | 671744/1648877 [00:16<00:34, 28723.21it/s]\n",
      "9920512it [04:50, 31671.86it/s]                                                                                        \n",
      " 42%|█████████████████████████████▏                                        | 688128/1648877 [00:17<00:30, 31267.57it/s]\n",
      " 42%|█████████████████████████████▌                                        | 696320/1648877 [00:17<00:30, 31137.37it/s]\n",
      " 43%|█████████████████████████████▉                                        | 704512/1648877 [00:17<00:29, 32175.18it/s]\n",
      " 43%|██████████████████████████████▎                                       | 712704/1648877 [00:18<00:28, 32350.16it/s]\n",
      " 44%|██████████████████████████████▌                                       | 720896/1648877 [00:18<00:37, 24938.78it/s]\n",
      " 44%|██████████████████████████████▉                                       | 729088/1648877 [00:18<00:32, 28439.52it/s]\n",
      " 45%|███████████████████████████████▎                                      | 737280/1648877 [00:19<00:32, 27980.13it/s]\n",
      " 45%|███████████████████████████████▋                                      | 745472/1648877 [00:19<00:29, 30671.43it/s]\n",
      " 46%|███████████████████████████████▉                                      | 753664/1648877 [00:19<00:28, 31549.23it/s]\n",
      " 46%|████████████████████████████████▎                                     | 761856/1648877 [00:19<00:27, 31694.58it/s]\n",
      " 47%|████████████████████████████████▋                                     | 770048/1648877 [00:20<00:35, 24861.59it/s]\n",
      " 47%|█████████████████████████████████                                     | 778240/1648877 [00:20<00:27, 31295.57it/s]\n",
      " 48%|█████████████████████████████████▍                                    | 786432/1648877 [00:20<00:33, 25491.73it/s]\n",
      " 48%|█████████████████████████████████▋                                    | 794624/1648877 [00:21<00:39, 21673.65it/s]\n",
      " 49%|██████████████████████████████████                                    | 802816/1648877 [00:21<00:41, 20524.76it/s]\n",
      " 49%|██████████████████████████████████▍                                   | 811008/1648877 [00:22<00:47, 17657.46it/s]\n",
      " 50%|███████████████████████████████████▏                                  | 827392/1648877 [00:23<00:44, 18460.76it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████▍                                  | 835584/1648877 [00:23<00:44, 18202.04it/s]\n",
      " 51%|███████████████████████████████████▊                                  | 843776/1648877 [00:24<00:45, 17638.98it/s]\n",
      " 52%|████████████████████████████████████▏                                 | 851968/1648877 [00:24<00:46, 17280.17it/s]\n",
      " 52%|████████████████████████████████████▌                                 | 860160/1648877 [00:24<00:40, 19454.45it/s]\n",
      " 53%|████████████████████████████████████▊                                 | 868352/1648877 [00:25<00:37, 20963.30it/s]\n",
      " 53%|█████████████████████████████████████▏                                | 876544/1648877 [00:25<00:34, 22599.82it/s]\n",
      " 54%|█████████████████████████████████████▌                                | 884736/1648877 [00:25<00:30, 24703.07it/s]\n",
      " 54%|█████████████████████████████████████▉                                | 892928/1648877 [00:26<00:28, 26838.02it/s]\n",
      " 55%|██████████████████████████████████████▎                               | 901120/1648877 [00:26<00:26, 28251.20it/s]\n",
      " 55%|██████████████████████████████████████▌                               | 909312/1648877 [00:26<00:21, 34044.61it/s]\n",
      " 56%|██████████████████████████████████████▉                               | 917504/1648877 [00:26<00:21, 33920.18it/s]\n",
      " 56%|███████████████████████████████████████▎                              | 925696/1648877 [00:26<00:20, 35000.49it/s]\n",
      " 57%|███████████████████████████████████████▉                              | 942080/1648877 [00:27<00:17, 39481.47it/s]\n",
      " 58%|████████████████████████████████████████▎                             | 950272/1648877 [00:27<00:15, 44318.19it/s]\n",
      " 58%|████████████████████████████████████████▋                             | 958464/1648877 [00:27<00:13, 49318.95it/s]\n",
      " 59%|█████████████████████████████████████████                             | 966656/1648877 [00:27<00:12, 53552.78it/s]\n",
      " 59%|█████████████████████████████████████████▍                            | 974848/1648877 [00:27<00:11, 57001.75it/s]\n",
      " 60%|██████████████████████████████████████████                            | 991232/1648877 [00:28<00:13, 48406.39it/s]\n",
      " 62%|██████████████████████████████████████████▊                          | 1024000/1648877 [00:28<00:10, 59477.33it/s]\n",
      " 63%|███████████████████████████████████████████▌                         | 1040384/1648877 [00:28<00:10, 60596.03it/s]\n",
      " 64%|███████████████████████████████████████████▉                         | 1048576/1648877 [00:29<00:15, 37602.93it/s]\n",
      " 65%|████████████████████████████████████████████▉                        | 1073152/1648877 [00:29<00:11, 50093.04it/s]\n",
      " 66%|█████████████████████████████████████████████▌                       | 1089536/1648877 [00:29<00:12, 45479.96it/s]\n",
      " 67%|██████████████████████████████████████████████▎                      | 1105920/1648877 [00:29<00:11, 48813.28it/s]\n",
      " 68%|██████████████████████████████████████████████▉                      | 1122304/1648877 [00:30<00:12, 41704.02it/s]\n",
      " 69%|███████████████████████████████████████████████▋                     | 1138688/1648877 [00:30<00:10, 47148.30it/s]\n",
      " 70%|███████████████████████████████████████████████▉                     | 1146880/1648877 [00:31<00:16, 30046.52it/s]\n",
      " 71%|████████████████████████████████████████████████▋                    | 1163264/1648877 [00:31<00:12, 37992.28it/s]\n",
      " 71%|█████████████████████████████████████████████████                    | 1171456/1648877 [00:31<00:14, 33901.02it/s]\n",
      " 72%|█████████████████████████████████████████████████▎                   | 1179648/1648877 [00:32<00:18, 25482.59it/s]\n",
      " 73%|██████████████████████████████████████████████████                   | 1196032/1648877 [00:32<00:17, 25561.55it/s]\n",
      " 73%|██████████████████████████████████████████████████▍                  | 1204224/1648877 [00:33<00:19, 22957.23it/s]\n",
      " 74%|██████████████████████████████████████████████████▋                  | 1212416/1648877 [00:33<00:18, 22993.31it/s]\n",
      " 74%|███████████████████████████████████████████████████                  | 1220608/1648877 [00:34<00:20, 20488.54it/s]\n",
      " 75%|███████████████████████████████████████████████████▍                 | 1228800/1648877 [00:34<00:22, 18519.80it/s]\n",
      " 75%|███████████████████████████████████████████████████▊                 | 1236992/1648877 [00:35<00:31, 13232.72it/s]\n",
      " 76%|████████████████████████████████████████████████████                 | 1245184/1648877 [00:36<00:29, 13640.71it/s]\n",
      " 76%|████████████████████████████████████████████████████▍                | 1253376/1648877 [00:36<00:28, 13820.07it/s]\n",
      " 77%|████████████████████████████████████████████████████▊                | 1261568/1648877 [00:37<00:25, 15365.45it/s]\n",
      " 77%|█████████████████████████████████████████████████████▏               | 1269760/1648877 [00:37<00:22, 16596.44it/s]\n",
      " 78%|█████████████████████████████████████████████████████▍               | 1277952/1648877 [00:37<00:18, 19637.04it/s]\n",
      " 78%|█████████████████████████████████████████████████████▊               | 1286144/1648877 [00:38<00:16, 22299.72it/s]\n",
      " 78%|██████████████████████████████████████████████████████▏              | 1294336/1648877 [00:38<00:17, 20197.28it/s]\n",
      " 79%|██████████████████████████████████████████████████████▊              | 1310720/1648877 [00:38<00:13, 25645.36it/s]\n",
      " 80%|███████████████████████████████████████████████████████▏             | 1318912/1648877 [00:39<00:15, 21175.21it/s]\n",
      " 80%|███████████████████████████████████████████████████████▌             | 1327104/1648877 [00:39<00:12, 25471.62it/s]\n",
      " 81%|███████████████████████████████████████████████████████▉             | 1335296/1648877 [00:40<00:15, 20114.34it/s]\n",
      " 81%|████████████████████████████████████████████████████████▏            | 1343488/1648877 [00:40<00:13, 22825.74it/s]\n",
      " 82%|████████████████████████████████████████████████████████▌            | 1351680/1648877 [00:40<00:13, 22162.07it/s]\n",
      " 82%|████████████████████████████████████████████████████████▉            | 1359872/1648877 [00:41<00:12, 22474.79it/s]\n",
      " 83%|█████████████████████████████████████████████████████████▏           | 1368064/1648877 [00:41<00:13, 20121.73it/s]\n",
      " 83%|█████████████████████████████████████████████████████████▌           | 1376256/1648877 [00:41<00:12, 21920.12it/s]\n",
      " 84%|█████████████████████████████████████████████████████████▉           | 1384448/1648877 [00:42<00:12, 21152.03it/s]\n",
      " 84%|██████████████████████████████████████████████████████████▎          | 1392640/1648877 [00:42<00:11, 21462.18it/s]\n",
      " 85%|██████████████████████████████████████████████████████████▌          | 1400832/1648877 [00:43<00:11, 21592.88it/s]\n",
      " 85%|██████████████████████████████████████████████████████████▉          | 1409024/1648877 [00:43<00:11, 20339.93it/s]\n",
      " 86%|███████████████████████████████████████████████████████████▎         | 1417216/1648877 [00:44<00:12, 19077.20it/s]\n",
      " 86%|███████████████████████████████████████████████████████████▋         | 1425408/1648877 [00:44<00:11, 18959.24it/s]\n",
      " 87%|███████████████████████████████████████████████████████████▉         | 1433600/1648877 [00:44<00:10, 20127.09it/s]\n",
      " 87%|████████████████████████████████████████████████████████████▎        | 1441792/1648877 [00:45<00:09, 22594.00it/s]\n",
      " 88%|████████████████████████████████████████████████████████████▋        | 1449984/1648877 [00:45<00:08, 23156.45it/s]\n",
      " 88%|█████████████████████████████████████████████████████████████        | 1458176/1648877 [00:46<00:09, 19341.73it/s]\n",
      " 89%|█████████████████████████████████████████████████████████████▋       | 1474560/1648877 [00:46<00:07, 22764.04it/s]\n",
      " 90%|██████████████████████████████████████████████████████████████       | 1482752/1648877 [00:46<00:06, 24918.48it/s]\n",
      " 90%|██████████████████████████████████████████████████████████████▍      | 1490944/1648877 [00:46<00:05, 27172.99it/s]\n",
      " 91%|██████████████████████████████████████████████████████████████▋      | 1499136/1648877 [00:47<00:05, 28583.15it/s]\n",
      " 91%|███████████████████████████████████████████████████████████████      | 1507328/1648877 [00:47<00:04, 29234.30it/s]\n",
      " 92%|███████████████████████████████████████████████████████████████▍     | 1515520/1648877 [00:47<00:04, 30480.41it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████▊     | 1523712/1648877 [00:47<00:04, 31139.33it/s]\n",
      " 93%|████████████████████████████████████████████████████████████████     | 1531904/1648877 [00:48<00:03, 31406.32it/s]\n",
      " 94%|████████████████████████████████████████████████████████████████▊    | 1548288/1648877 [00:48<00:02, 37273.38it/s]\n",
      " 94%|█████████████████████████████████████████████████████████████████▏   | 1556480/1648877 [00:48<00:02, 35535.58it/s]\n",
      " 95%|█████████████████████████████████████████████████████████████████▍   | 1564672/1648877 [00:48<00:02, 34415.05it/s]\n",
      " 96%|██████████████████████████████████████████████████████████████████▏  | 1581056/1648877 [00:49<00:01, 40488.02it/s]\n",
      " 97%|██████████████████████████████████████████████████████████████████▊  | 1597440/1648877 [00:49<00:01, 45596.82it/s]\n",
      " 97%|███████████████████████████████████████████████████████████████████▏ | 1605632/1648877 [00:49<00:01, 40097.34it/s]\n",
      " 98%|███████████████████████████████████████████████████████████████████▉ | 1622016/1648877 [00:49<00:00, 45640.44it/s]\n",
      " 99%|████████████████████████████████████████████████████████████████████▌| 1638400/1648877 [00:50<00:00, 40479.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "  0%|                                                                                         | 0/4542 [00:00<?, ?it/s]\n",
      "\n",
      "8192it [00:00, 15365.30it/s]                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "mean, std = 0.1307, 0.3081\n",
    "\n",
    "train_dataset = MNIST('./data/MNIST', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((mean,), (std,))\n",
    "                             ]))\n",
    "test_dataset = MNIST('./data/MNIST', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                            ]))\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcZTFRnjCut3"
   },
   "source": [
    "## Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Dz2xh66UCut5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from trainer import fit\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(10):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(mnist_classes)\n",
    "\n",
    "def extract_embeddings(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), 2))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75moY8AyCut_"
   },
   "source": [
    "# Baseline: Classification with softmax\n",
    "We'll train the model for classification and use outputs of penultimate layer as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "95Q40yq0CuuB"
   },
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "batch_size = 256\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet, ClassificationNet\n",
    "from metrics import AccumulatedAccuracyMetric\n",
    "\n",
    "embedding_net = EmbeddingNet()\n",
    "model = ClassificationNet(embedding_net, n_classes=n_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 2\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "De3Wt6CCCuuG",
    "outputId": "717714e8-e267-4ea6-f832-5d186fee6eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/60000 (0%)]\tLoss: 2.379158\tAccuracy: 11.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1654784it [01:07, 40479.76it/s]                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [12800/60000 (21%)]\tLoss: 3.360720\tAccuracy: 12.691482843137255\n",
      "Train: [25600/60000 (43%)]\tLoss: 1.954630\tAccuracy: 19.179300742574256\n",
      "Train: [38400/60000 (64%)]\tLoss: 1.520945\tAccuracy: 26.479718543046356\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.841263\tAccuracy: 37.59717039800995\n",
      "Epoch: 1/20. Train set: Average loss: 1.7189\tAccuracy: 44.78\n",
      "Epoch: 1/20. Validation set: Average loss: 0.5009\tAccuracy: 89.22\n",
      "Train: [0/60000 (0%)]\tLoss: 0.442388\tAccuracy: 88.671875\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.376264\tAccuracy: 92.76960784313725\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.273753\tAccuracy: 93.80414603960396\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.253985\tAccuracy: 94.21047185430463\n"
     ]
    }
   ],
   "source": [
    "fit(train_loader, test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AccumulatedAccuracyMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "q9o-6-ytCuuT",
    "outputId": "bc12c8be-999e-4329-91b1-83fb2395df0c"
   },
   "outputs": [],
   "source": [
    "train_embeddings_baseline, train_labels_baseline = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_baseline, train_labels_baseline)\n",
    "val_embeddings_baseline, val_labels_baseline = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_baseline, val_labels_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDdsCIiHCuub"
   },
   "source": [
    "While the embeddings look separable (which is what we trained them for), they don't have good metric properties. They might not be the best choice as a descriptor for new classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9FhHE-tCuuc"
   },
   "source": [
    "# Siamese network\n",
    "Now we'll train a siamese network that takes a pair of images and trains the embeddings so that the distance between them is minimized if their from the same class or greater than some margin value if they represent different classes.\n",
    "We'll minimize a contrastive loss function*:\n",
    "$$L_{contrastive}(x_0, x_1, y) = \\frac{1}{2} y \\lVert f(x_0)-f(x_1)\\rVert_2^2 + \\frac{1}{2}(1-y)\\{max(0, m-\\lVert f(x_0)-f(x_1)\\rVert_2)\\}^2$$\n",
    "\n",
    "*Raia Hadsell, Sumit Chopra, Yann LeCun, [Dimensionality reduction by learning an invariant mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf), CVPR 2006*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-qpIq-TzCuue"
   },
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "from datasets import SiameseMNIST\n",
    "\n",
    "siamese_train_dataset = SiameseMNIST(train_dataset) # Returns pairs of images and target same/different\n",
    "siamese_test_dataset = SiameseMNIST(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet, SiameseNet\n",
    "from losses import ContrastiveLoss\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = SiameseNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = ContrastiveLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "_IqiBATeCuuh",
    "outputId": "c9e31172-3754-4ff2-8b7f-1bc2720d554b"
   },
   "outputs": [],
   "source": [
    "fit(siamese_train_loader, siamese_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "rBBBuPjiCuup",
    "outputId": "0849de03-297f-4189-d5ea-fc8c5fda27e1"
   },
   "outputs": [],
   "source": [
    "train_embeddings_cl, train_labels_cl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_cl, train_labels_cl)\n",
    "val_embeddings_cl, val_labels_cl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_cl, val_labels_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbKXy6yQCuuu"
   },
   "source": [
    "# Triplet network\n",
    "We'll train a triplet network, that takes an anchor, positive (same class as anchor) and negative (different class than anchor) examples. The objective is to learn embeddings such that the anchor is closer to the positive example than it is to the negative example by some margin value.\n",
    "\n",
    "![alt text](images/anchor_negative_positive.png \"Source: FaceNet\")\n",
    "Source: [2] *Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [Facenet: A unified embedding for face recognition and clustering.](https://arxiv.org/abs/1503.03832) CVPR 2015.*\n",
    "\n",
    "**Triplet loss**:   $L_{triplet}(x_a, x_p, x_n) = max(0, m +  \\lVert f(x_a)-f(x_p)\\rVert_2^2 - \\lVert f(x_a)-f(x_n)\\rVert_2^2$\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jv4DvFucCuuu"
   },
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "from datasets import TripletMNIST\n",
    "\n",
    "triplet_train_dataset = TripletMNIST(train_dataset) # Returns triplets of images\n",
    "triplet_test_dataset = TripletMNIST(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet, TripletNet\n",
    "from losses import TripletLoss\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = TripletNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "Dj9AoYpsCuuz",
    "outputId": "70ff7e3d-4e0b-403c-9af1-41c51a4c808e"
   },
   "outputs": [],
   "source": [
    "fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "ysh4Ry7ZCuu_",
    "outputId": "4194cf1d-da83-452a-94d6-2cd4c31aca2a"
   },
   "outputs": [],
   "source": [
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_tl, train_labels_tl)\n",
    "val_embeddings_tl, val_labels_tl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_tl, val_labels_tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7C9H1_nCuvJ"
   },
   "source": [
    "# Online pair/triplet selection - negative mining\n",
    "There are couple of problems with siamese and triplet networks.\n",
    "1. The number of possible pairs/triplets grows **quadratically/cubically** with the number of examples. It's infeasible to process them all\n",
    "2. We generate pairs/triplets randomly. As the training continues, more and more pairs/triplets are easy to deal with (their loss value is very small or even 0), preventing the network from training. We need to provide the network with **hard examples**.\n",
    "3. Each image that is fed to the network is used only for computation of contrastive/triplet loss for only one pair/triplet. The computation is somewhat wasted; once the embedding is computed, it could be reused for many pairs/triplets.\n",
    "\n",
    "To deal with that efficiently, we'll feed a network with standard mini-batches as we did for classification. The loss function will be responsible for selection of hard pairs and triplets within mini-batch. In these case, if we feed the network with 16 images per 10 classes, we can process up to $159*160/2 = 12720$ pairs and $10*16*15/2*(9*16) = 172800$ triplets, compared to 80 pairs and 53 triplets in previous implementation.\n",
    "\n",
    "We can find some strategies on how to select triplets in [2] and [3] *Alexander Hermans, Lucas Beyer, Bastian Leibe, [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/pdf/1703.07737), 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k806qej9CuvL"
   },
   "source": [
    "## Online pair selection\n",
    "## Steps\n",
    "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
    "2. Create data loaders with the batch sampler\n",
    "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "4. Define a **PairSelector** that takes embeddings and original labels and returns valid pairs within a minibatch\n",
    "5. Define **OnlineContrastiveLoss** that will use a *PairSelector* and compute *ContrastiveLoss* on such pairs\n",
    "6. Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "goVi1_-PCuvL"
   },
   "outputs": [],
   "source": [
    "from datasets import BalancedBatchSampler\n",
    "\n",
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet\n",
    "from losses import OnlineContrastiveLoss\n",
    "from utils import AllPositivePairSelector, HardNegativePairSelector # Strategies for selecting pairs within a minibatch\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineContrastiveLoss(margin, HardNegativePairSelector())\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "SacMTqN6CuvO",
    "outputId": "d339ea0e-a143-423f-939a-55f0b9a38e6a"
   },
   "outputs": [],
   "source": [
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "ZCvF0AlCCuvX",
    "outputId": "84044105-6289-4a9f-9ec5-47a7d539651a"
   },
   "outputs": [],
   "source": [
    "train_embeddings_ocl, train_labels_ocl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_ocl, train_labels_ocl)\n",
    "val_embeddings_ocl, val_labels_ocl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_ocl, val_labels_ocl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgIGiMwICuvn"
   },
   "source": [
    "## Online triplet selection\n",
    "## Steps\n",
    "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
    "2. Create data loaders with the batch sampler\n",
    "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "4. Define a **TripletSelector** that takes embeddings and original labels and returns valid triplets within a minibatch\n",
    "5. Define **OnlineTripletLoss** that will use a *TripletSelector* and compute *TripletLoss* on such pairs\n",
    "6. Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JzpYzMUuCuvp"
   },
   "outputs": [],
   "source": [
    "from datasets import BalancedBatchSampler\n",
    "\n",
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet\n",
    "from losses import OnlineTripletLoss\n",
    "from utils import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
    "from metrics import AverageNonzeroTripletsMetric\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineTripletLoss(margin, RandomNegativeTripletSelector(margin))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2397
    },
    "colab_type": "code",
    "id": "W-bDxqVJCuvs",
    "outputId": "0b3f7143-d6e5-4f74-9e70-17405026cc91"
   },
   "outputs": [],
   "source": [
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "dNrbA2hCCuvw",
    "outputId": "df17047b-8a35-4370-ebe7-be83e9154003"
   },
   "outputs": [],
   "source": [
    "train_embeddings_otl, train_labels_otl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_otl, train_labels_otl)\n",
    "val_embeddings_otl, val_labels_otl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_otl, val_labels_otl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "tNY7NReM0zsq",
    "outputId": "5c4db690-d4db-4182-c5d6-9c8d6fc816e7"
   },
   "outputs": [],
   "source": [
    "# display_emb_online, display_emb, display_label_online, display_label = train_embeddings_otl, train_embeddings_tl, train_labels_otl, train_labels_tl\n",
    "display_emb_online, display_emb, display_label_online, display_label = val_embeddings_otl, val_embeddings_tl, val_labels_otl, val_labels_tl\n",
    "x_lim = (np.min(display_emb_online[:,0]), np.max(display_emb_online[:,0]))\n",
    "y_lim = (np.min(display_emb_online[:,1]), np.max(display_emb_online[:,1]))\n",
    "plot_embeddings(display_emb, display_label, x_lim, y_lim)\n",
    "plot_embeddings(display_emb_online, display_label_online, x_lim, y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1167
    },
    "colab_type": "code",
    "id": "-vyd_460yt7n",
    "outputId": "68c58b88-059f-4b04-ddee-3d3a59bc1856"
   },
   "outputs": [],
   "source": [
    "x_lim = (np.min(train_embeddings_ocl[:,0]), np.max(train_embeddings_ocl[:,0]))\n",
    "y_lim = (np.min(train_embeddings_ocl[:,1]), np.max(train_embeddings_ocl[:,1]))\n",
    "plot_embeddings(train_embeddings_cl, train_labels_cl, x_lim, y_lim)\n",
    "plot_embeddings(train_embeddings_ocl, train_labels_ocl, x_lim, y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "smvUHdXOyt7r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Experiments_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
